import os
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# –ü–∞–ø–∫–∞ —Å —Ñ–∞–π–ª–∞–º–∏
DATA_FOLDER = "/Users/timskeip/Downloads/data/ILCD/processed"

# –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ FAISS
dimension = 384  # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (MiniLM)
index = faiss.IndexFlatL2(dimension)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º L2 —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ

# –°–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
texts = []
metadata = []

# –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª—ã
for i, filename in enumerate(os.listdir(DATA_FOLDER)):
    if filename.endswith('.txt'):
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—Å—Ç
        txt_path = os.path.join(DATA_FOLDER, filename)
        with open(txt_path, 'r', encoding='utf-8') as f:
            text = f.read().strip()

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π —ç–º–±–µ–¥–¥–∏–Ω–≥
        npy_path = os.path.join(DATA_FOLDER, filename.replace('.txt', '.npy'))
        if not os.path.exists(npy_path):
            print(f"‚ö†Ô∏è –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ–∞–π–ª–∞ {filename} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
            continue

        embedding = np.load(npy_path).astype('float32')  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ float32

        # –î–æ–±–∞–≤–ª—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –≤ –∏–Ω–¥–µ–∫—Å FAISS
        index.add(np.array([embedding]))

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—Å—Ç –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        texts.append(text)
        metadata.append({
            "filename": filename,
            "id": i  # –£–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        })

print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(texts)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ FAISS!")

# –§—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞
def search_faiss(query, top_k=3):
    # –°–æ–∑–¥–∞—ë–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
    query_embedding = model.encode(query).astype('float32')

    # –ü–æ–∏—Å–∫ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π
    distances, indices = index.search(np.array([query_embedding]), top_k)

    print("\nüîç **–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞:**")
    for i, idx in enumerate(indices[0]):
        print(f"\n[{i + 1}] üìÑ **{metadata[idx]['filename']}** (–†–∞—Å—Å—Ç–æ—è–Ω–∏–µ: {distances[0][i]:.4f})")
        print(f"üìù {texts[idx][:300]}...")
        print("-" * 50)

# –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ–∏—Å–∫
query = "Gas DE?"
search_faiss(query)




# import os
# import numpy as np
# from qdrant_client import QdrantClient
# from qdrant_client.models import Distance, VectorParams, PointStruct
# import faiss
#
# # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ Qdrant
# client = QdrantClient(
#     url="",
#     api_key="",
# )
#
# # –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
# COLLECTION_NAME = "processes"
#
# # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∫–æ–ª–ª–µ–∫—Ü–∏—è, –∏ —Å–æ–∑–¥–∞—ë–º –µ—ë, –µ—Å–ª–∏ –Ω–µ—Ç
# collections = client.get_collections().collections
# if COLLECTION_NAME not in [c.name for c in collections]:
#     client.recreate_collection(
#         collection_name=COLLECTION_NAME,
#         vectors_config=VectorParams(
#             size=384,  # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (MiniLM)
#             distance=Distance.COSINE  # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞
#         )
#     )
#     print("‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞!")
# else:
#     print("‚ö° –ö–æ–ª–ª–µ–∫—Ü–∏—è —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ.")
#
# # –ü–∞–ø–∫–∞ —Å —Ñ–∞–π–ª–∞–º–∏
# DATA_FOLDER = ""
#
# # –°–ø–∏—Å–æ–∫ –¥–ª—è —Ç–æ—á–µ–∫ Qdrant
# points = []
#
# # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª—ã
# for i, filename in enumerate(os.listdir(DATA_FOLDER)):
#     if filename.endswith('.txt'):
#         # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—Å—Ç
#         txt_path = os.path.join(DATA_FOLDER, filename)
#         with open(txt_path, 'r', encoding='utf-8') as f:
#             text = f.read().strip()
#
#         # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π —ç–º–±–µ–¥–¥–∏–Ω–≥
#         npy_path = os.path.join(DATA_FOLDER, filename.replace('.txt', '.npy'))
#         if not os.path.exists(npy_path):
#             print(f"‚ö†Ô∏è –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ–∞–π–ª–∞ {filename} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
#             continue
#
#         embedding = np.load(npy_path).tolist()  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ø–∏—Å–æ–∫
#
#         # –°–æ–∑–¥–∞—ë–º —Ç–æ—á–∫—É –¥–ª—è Qdrant
#         point = PointStruct(
#             id=i,
#             vector=embedding,
#             payload={
#                 "filename": filename,
#                 "text": text,
#             }
#         )
#         points.append(point)
#
# # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –≤ Qdrant
# if points:
#     client.upsert(collection_name=COLLECTION_NAME, points=points)
#     print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(points)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ Qdrant!")
# else:
#     print("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤ Qdrant.")
#
# # –§—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞
# def search_qdrant(query, top_k=3):
#     # –ï—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –µ—ë
#     # query_embedding = model.encode(query).tolist()
#     # –ò–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞ –∏–∑ —Ñ–∞–π–ª–∞, –µ—Å–ª–∏ –æ–Ω —É –≤–∞—Å –µ—Å—Ç—å
#
#     # –ü—Ä–∏–º–µ—Ä: –ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —á—Ç–æ —É –≤–∞—Å –µ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
#     query_embedding = [...]  # –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –≤–∞—à —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
#
#     search_results = client.search(
#         collection_name=COLLECTION_NAME,
#         query_vector=query_embedding,
#         limit=top_k
#     )
#
#     print("\nüîç **–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞:**")
#     for i, result in enumerate(search_results, 1):
#         print(f"\n[{i}] üìÑ **{result.payload['filename']}** (Score: {result.score:.4f})")
#         print(f"üìù {result.payload['text'][:300]}...")
#         print("-" * 50)
#
# # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ–∏—Å–∫
# query = "Gas DE?"
# search_qdrant(query)
