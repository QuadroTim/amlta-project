{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPQ5zZoJOa+Nd1L7tCvutvN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/woranov/amlta-project/blob/main/notebooks/app_in_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MnByl0VBlkzK"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import amlta  # noqa: F401\n",
        "except ImportError:\n",
        "    %pip install -q --no-dependencies -U git+https://github.com/woranov/amlta-project.git\n",
        "    %pip install -q streamlit fastembed langchain langchain-community langchain-ollama langchain-qdrant langgraph colab-xterm pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext colabxterm"
      ],
      "metadata": {
        "id": "87q2VCJBm_Wb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCLdjXOdnQzf",
        "outputId": "6934f9a3-4302-4cff-d543-47377343b865"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "############################################################################################# 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# HOST=0.0.0.0 ollama serve\n",
        "%xterm"
      ],
      "metadata": {
        "id": "suplDqihn_40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull qwen2.5-coder:14b"
      ],
      "metadata": {
        "id": "fGCJZYcWnhZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_ollama import ChatOllama\n",
        "\n",
        "ChatOllama(model=\"qwen2.5-coder:14b\").invoke(\"hello :)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQZElUQq1FEm",
        "outputId": "e19f3e27-ed45-44a2-901c-d43f5c79d318"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hello! How can I assist you today? Feel free to ask any questions or let me know if there's anything specific you'd like help with.\", additional_kwargs={}, response_metadata={'model': 'qwen2.5-coder:14b', 'created_at': '2025-02-18T16:15:04.215907491Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1861870659, 'load_duration': 74362708, 'prompt_eval_count': 31, 'prompt_eval_duration': 173000000, 'eval_count': 31, 'eval_duration': 1601000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-391fcb91-fd7f-4a21-b13b-9c320ed132fe-0', usage_metadata={'input_tokens': 31, 'output_tokens': 31, 'total_tokens': 62})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!amlta-app -- --model \"qwen2.5-coder:14b\" &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "Gr36rH2DsTdl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "from google.colab import userdata\n",
        "\n",
        "# put ngrok token in colab secrets\n",
        "# https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "ngrok.set_auth_token(userdata.get(\"NGROK_AUTHTOKEN\"))\n",
        "ngrok_tunnel = ngrok.connect(addr=\"8501\", proto=\"http\", bind_tls=True)\n",
        "\n",
        "# Print the URL of the ngrok tunnel\n",
        "print(' * Tunnel URL:', ngrok_tunnel.public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeUQMxapscdy",
        "outputId": "f66fefb9-a3d5-494e-f6f9-d04421ad4611"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Tunnel URL: https://a43f-35-186-154-183.ngrok-free.app\n"
          ]
        }
      ]
    }
  ]
}