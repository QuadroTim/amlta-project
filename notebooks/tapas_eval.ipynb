{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import amlta  # noqa: F401\n",
    "except ImportError:\n",
    "    %pip install -q --no-dependencies -U git+https://github.com/woranov/amlta-project.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from google.colab import drive  # pyright: ignore[reportMissingImports]\n",
    "\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "from amlta.config import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    mount_point = Path(\"/content/drive\")\n",
    "    drive_path = mount_point / \"MyDrive\"\n",
    "\n",
    "    # edit\n",
    "    data_dir = drive_path / \"uni\" / \"ws2425\" / \"amlta\" / \"project\" / \"data\"\n",
    "\n",
    "    config.update(data_dir=data_dir)\n",
    "\n",
    "    if not mount_point.exists():\n",
    "        drive.mount(str(mount_point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm.contrib.concurrent import thread_map\n",
    "\n",
    "from amlta.probas.flows import extract_process_flows\n",
    "from amlta.probas.processes import ProcessData\n",
    "from amlta.question_generation.process import (\n",
    "    QuestionData,\n",
    "    load_batches,\n",
    ")\n",
    "from amlta.question_generation.query_params import get_flows_for_query\n",
    "from amlta.tapas.model import load_tapas_model, load_tapas_tokenizer\n",
    "from amlta.tapas.retrieve import retrieve_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = pd.read_parquet(\n",
    "    config.data_dir / \"tapas-ft\" / \"data\" / \"tapas_train_batched_dfs_shuffled.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_trained_on_data = training_df.iloc[int(len(training_df) * 0.8) :]\n",
    "start_batch = not_trained_on_data[\"batch\"].values[0]\n",
    "start_question_id = int(not_trained_on_data[\"question_id\"].values[0])\n",
    "start_process_uuid = not_trained_on_data[\"process_uuid\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'TapasTokenizer'. \n",
      "The class this function is called from is 'CustomTapasTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = load_tapas_tokenizer()\n",
    "model = load_tapas_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch': 'batch_gpt-4o-mini_800_1000', 'process_uuid': 'ecf185cb-c44b-4450-abc4-a92c4bf0b9b9', 'question_id': 2, 'basic_query': 'metal production', 'general_query': 'non-ferrous metal production', 'specific_query': 'aluminum production in Germany 2015', 'flow_query_params': {'query_type': 'names', 'direction': 'output', 'aggregation': 'count', 'flow_names': ['HFC-245ca', 'perfluoropropane', 'CFC-11', 'nickel']}, 'question': 'What are the output amounts of HFC-245ca, perfluoropropane, CFC-11, and nickel from <the process>?', 'question_replaced_basic': 'What are the output amounts of HFC-245ca, perfluoropropane, CFC-11, and nickel from metal production?', 'question_replaced_general': 'What are the output amounts of HFC-245ca, perfluoropropane, CFC-11, and nickel from non-ferrous metal production?', 'question_replaced_specific': 'What are the output amounts of HFC-245ca, perfluoropropane, CFC-11, and nickel from aluminum production in Germany 2015?'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_data = load_batches()\n",
    "print(question_data[-1])\n",
    "\n",
    "len(question_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "722"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_valid_idx = next(\n",
    "    i\n",
    "    for i, q in enumerate(question_data)\n",
    "    if q[\"batch\"] == start_batch\n",
    "    and q[\"question_id\"] == start_question_id\n",
    "    and q[\"process_uuid\"] == start_process_uuid\n",
    ")\n",
    "valid_data = question_data[start_valid_idx:]\n",
    "len(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "\n",
    "@functools.lru_cache(maxsize=512)\n",
    "def get_process(uuid):\n",
    "    return ProcessData.from_uuid(uuid)\n",
    "\n",
    "\n",
    "@functools.lru_cache(maxsize=512)\n",
    "def get_flows(uuid):\n",
    "    return extract_process_flows(get_process(uuid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_labels(question: QuestionData):\n",
    "    df = get_flows(question[\"process_uuid\"])\n",
    "\n",
    "    filtered = get_flows_for_query(df, question[\"flow_query_params\"])\n",
    "\n",
    "    labels = df.index.isin(filtered.index).astype(bool).tolist()\n",
    "    aggregation = question[\"flow_query_params\"][\"aggregation\"]\n",
    "\n",
    "    aggregation = aggregation.replace(\"list\", \"NONE\").upper()\n",
    "\n",
    "    return labels, aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tapas_labels(question: QuestionData, threshold=threshold):\n",
    "    df = get_flows(question[\"process_uuid\"])\n",
    "\n",
    "    query = question[\"question\"]\n",
    "    query = query.replace(\"<\", \"\").replace(\">\", \"\")\n",
    "\n",
    "    rows, aggregation, probs = retrieve_rows(\n",
    "        df,\n",
    "        query=query,\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        threshold=0,\n",
    "        return_probabilities=True,\n",
    "    )\n",
    "    rows_sorted = []\n",
    "    probs_sorted = []\n",
    "\n",
    "    for row, prob in sorted(zip(rows, probs), key=lambda x: x[0]):\n",
    "        rows_sorted.append(row)\n",
    "        probs_sorted.append(prob)\n",
    "\n",
    "    labels = [bool(prob >= threshold) for prob in probs_sorted]\n",
    "\n",
    "    return labels, aggregation, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05f5323a82a64dc5befd879bc1de3740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_preds_labels = []\n",
    "y_preds_probs = []\n",
    "y_preds_aggregation = []\n",
    "\n",
    "for res in thread_map(get_tapas_labels, valid_data[:3], max_workers=8):\n",
    "    labels, aggregation, probs = res\n",
    "    y_preds_labels.append(labels)\n",
    "    y_preds_probs.append(probs)\n",
    "    y_preds_aggregation.append(aggregation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc776c9fb4fd425bae73956fddce5f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_trues_labels = []\n",
    "y_trues_aggregation = []\n",
    "\n",
    "for res in thread_map(get_true_labels, valid_data[:3], max_workers=8):\n",
    "    labels, aggregation = res\n",
    "    y_trues_labels.append(labels)\n",
    "    y_trues_aggregation.append(aggregation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      1.00      1.00       179\n",
      "        True       1.00      1.00      1.00        64\n",
      "\n",
      "    accuracy                           1.00       243\n",
      "   macro avg       1.00      1.00      1.00       243\n",
      "weighted avg       1.00      1.00      1.00       243\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        [label for labels in y_trues_labels for label in labels],\n",
    "        [label for labels in y_preds_labels for label in labels],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NONE', 'NONE', 'COUNT']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_trues_aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SUM', 'NONE', 'COUNT']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds_aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       COUNT       1.00      1.00      1.00         1\n",
      "        NONE       1.00      0.50      0.67         2\n",
      "         SUM       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.67         3\n",
      "   macro avg       0.67      0.50      0.56         3\n",
      "weighted avg       1.00      0.67      0.78         3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\dev\\amlta-project\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\dev\\amlta-project\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\dev\\amlta-project\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_trues_aggregation, y_preds_aggregation))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
