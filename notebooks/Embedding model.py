# import os
# import yaml
# import numpy as np
# from sentence_transformers import SentenceTransformer
#
# # –ü–∞–ø–∫–∏ —Å –¥–∞–Ω–Ω—ã–º–∏
# DATA_FOLDER = "/Users/timskeip/Downloads/data-2/ILCD/processes_yaml_2"
# OUTPUT_FOLDER = "/Users/timskeip/Downloads/data-2/ILCD/embedding_hugface"
# os.makedirs(OUTPUT_FOLDER, exist_ok=True)
#
# # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
# model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å
#
# # –§—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ YAML-—Ñ–∞–π–ª–∞
# def load_yaml(file_path):
#     with open(file_path, 'r', encoding='utf-8') as file:
#         return yaml.safe_load(file) or {}
#
# # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–ø–∏—Å–∫–æ–≤, —Å—Ç—Ä–æ–∫ –∏ —Å–ª–æ–≤–∞—Ä–µ–π –≤ YAML
# def process_yaml_value(value):
#     """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –ª—é–±—ã–µ —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –≤ —á–∏—Ç–∞–µ–º—ã–π —Ç–µ–∫—Å—Ç"""
#     if isinstance(value, list):
#         return " | ".join(process_yaml_value(v) for v in value)
#     elif isinstance(value, dict):
#         return " | ".join(f"{k}: {process_yaml_value(v)}" for k, v in value.items())
#     elif isinstance(value, str):
#         return value.strip()
#     return ""
#
# # –§—É–Ω–∫—Ü–∏—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ —Ç–µ–∫—Å—Ç–∞
# def prepare_text_for_rag(data):
#     text_parts = []
#
#     # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è
#     text_parts.append(f"Name: {data.get('Name', 'Unknown')}")
#     text_parts.append(f"Year: {process_yaml_value(data.get('Year', []))}")
#     text_parts.append(f"Geography: {data.get('Geography', 'Unknown')}")
#     text_parts.append(f"Class: {process_yaml_value(data.get('Class', []))}")
#     text_parts.append(f"Technology: {process_yaml_value(data.get('Technology', []))}")
#     text_parts.append(f"Main Output: {process_yaml_value(data.get('Main Output', []))}")
#
#     return " ".join(text_parts)
#
# # –û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å
# def main():
#     for filename in os.listdir(DATA_FOLDER):
#         if filename.endswith('.yaml'):
#             file_path = os.path.join(DATA_FOLDER, filename)
#             print(f"Processing file: {filename}")
#
#             # –ó–∞–≥—Ä—É–∂–∞–µ–º YAML
#             data = load_yaml(file_path)
#             if not data:
#                 print(f"‚ö†Ô∏è Warning: Empty or invalid YAML in {filename}")
#                 continue
#
#             # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞
#             text = prepare_text_for_rag(data)
#             if not text.strip():
#                 print(f"‚ö†Ô∏è Warning: Empty text generated for {filename}")
#                 continue
#
#             # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
#             embedding = model.encode(text).astype('float32')
#
#             # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
#             if embedding.shape[0] != 768:  # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–ª—è paraphrase-multilingual-mpnet-base-v2
#                 print(f"‚ö†Ô∏è Warning: Unexpected embedding dimension for {filename}: {embedding.shape[0]}")
#                 continue
#
#             # –ü—É—Ç–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
#             base_filename = os.path.splitext(filename)[0]
#             text_file_path = os.path.join(OUTPUT_FOLDER, f"{base_filename}.txt")
#             embedding_file_path = os.path.join(OUTPUT_FOLDER, f"{base_filename}.npy")
#
#             # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—Å—Ç
#             with open(text_file_path, "w", encoding="utf-8") as f:
#                 f.write(text)
#
#             # –°–æ—Ö—Ä–∞–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
#             np.save(embedding_file_path, embedding)
#
#             print(f"‚úÖ Saved: {text_file_path} and {embedding_file_path}")
#
# if __name__ == "__main__":
#     main()





import os
import numpy as np
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
from sentence_transformers import SentenceTransformer
from rank_bm25 import BM25Okapi
from sklearn.preprocessing import MinMaxScaler

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')

# –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ Qdrant –ª–æ–∫–∞–ª—å–Ω–æ
client = QdrantClient(":memory:")  # –ò—Å–ø–æ–ª—å–∑—É–µ–º in-memory Qdrant –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞

# –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
COLLECTION_NAME = "processes"

# –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∫–æ–ª–ª–µ–∫—Ü–∏—è, –∏ —Å–æ–∑–¥–∞—ë–º –µ—ë, –µ—Å–ª–∏ –Ω–µ—Ç
if not client.collection_exists(collection_name=COLLECTION_NAME):
    client.create_collection(
        collection_name=COLLECTION_NAME,
        vectors_config=VectorParams(
            size=768,  # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è paraphrase-multilingual-mpnet-base-v2
            distance=Distance.COSINE
        )
    )
    print("‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞!")
else:
    print("‚ö° –ö–æ–ª–ª–µ–∫—Ü–∏—è —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ.")

# –ü–∞–ø–∫–∞ —Å —Ñ–∞–π–ª–∞–º–∏
DATA_FOLDER = "/Users/timskeip/Downloads/data-2/ILCD/embedding_hugface"  # –£–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –∫ –≤–∞—à–µ–π –ø–∞–ø–∫–µ

# –°–ø–∏—Å–æ–∫ –¥–ª—è —Ç–æ—á–µ–∫ Qdrant
points = []
texts = []  # –°–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è BM25

# –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª—ã
for i, filename in enumerate(os.listdir(DATA_FOLDER)):
    if filename.endswith('.txt'):
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—Å—Ç
        txt_path = os.path.join(DATA_FOLDER, filename)
        with open(txt_path, 'r', encoding='utf-8') as f:
            text = f.read().strip()
        texts.append(text)  # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç –≤ —Å–ø–∏—Å–æ–∫ –¥–ª—è BM25

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π —ç–º–±–µ–¥–¥–∏–Ω–≥
        npy_path = os.path.join(DATA_FOLDER, filename.replace('.txt', '.npy'))
        if not os.path.exists(npy_path):
            print(f"‚ö†Ô∏è –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ–∞–π–ª–∞ {filename} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
            continue

        embedding = np.load(npy_path).tolist()  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ø–∏—Å–æ–∫

        # –°–æ–∑–¥–∞—ë–º —Ç–æ—á–∫—É –¥–ª—è Qdrant
        point = PointStruct(
            id=i,
            vector=embedding,
            payload={
                "filename": filename,
                "text": text,
            }
        )
        points.append(point)

# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –≤ Qdrant
if points:
    client.upsert(collection_name=COLLECTION_NAME, points=points)
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(points)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ Qdrant!")
else:
    print("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤ Qdrant.")

# –°–æ–∑–¥–∞—ë–º BM25 –∏–Ω–¥–µ–∫—Å
tokenized_texts = [text.split() for text in texts]  # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤
bm25 = BM25Okapi(tokenized_texts)  # –°–æ–∑–¥–∞—ë–º BM25 –∏–Ω–¥–µ–∫—Å

# –§—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Qdrant –∏ BM25
def search_qdrant_and_bm25(query, top_k=10):
    # –ü–æ–∏—Å–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Qdrant (—ç–º–±–µ–¥–¥–∏–Ω–≥–∏)
    query_embedding = model.encode(query).tolist()
    qdrant_results = client.search(
        collection_name=COLLECTION_NAME,
        query_vector=query_embedding,
        limit=top_k
    )

    print("\nüîç **–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ Qdrant:**")
    for i, result in enumerate(qdrant_results, 1):
        print(f"\n[{i}] üìÑ **{result.payload['filename']}** (Score: {result.score:.4f})")
        print(f"üìù {result.payload['text'][:300]}...")
        print("-" * 50)

    # –ü–æ–∏—Å–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º BM25
    tokenized_query = query.split()
    bm25_scores = bm25.get_scores(tokenized_query)
    top_bm25_indices = np.argsort(bm25_scores)[-top_k:][::-1]  # –ò–Ω–¥–µ–∫—Å—ã —Ç–æ–ø-K –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

    print("\nüîç **–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ BM25:**")
    for i, idx in enumerate(top_bm25_indices, 1):
        print(f"\n[{i}] üìÑ **{points[idx].payload['filename']}** (Score: {bm25_scores[idx]:.4f})")
        print(f"üìù {points[idx].payload['text'][:300]}...")
        print("-" * 50)


# –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫
def hybrid_search(query, alpha=0.5, top_k=10):
    query_embedding = model.encode(query).tolist()

    # 1. Qdrant –ø–æ–∏—Å–∫
    qdrant_results = client.search(
        collection_name=COLLECTION_NAME,
        query_vector=query_embedding,
        limit=top_k
    )

    qdrant_scores = {res.payload['filename']: res.score for res in qdrant_results}

    # 2. BM25 –ø–æ–∏—Å–∫
    tokenized_query = query.split()
    bm25_scores = bm25.get_scores(tokenized_query)
    top_bm25_indices = np.argsort(bm25_scores)[-top_k:][::-1]

    bm25_scores_dict = {points[idx].payload['filename']: bm25_scores[idx] for idx in top_bm25_indices}

    # 3. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    scaler = MinMaxScaler()
    all_scores = np.array(list(qdrant_scores.values()) + list(bm25_scores_dict.values())).reshape(-1, 1)
    normalized_scores = scaler.fit_transform(all_scores).flatten()

    # –†–∞–∑–¥–µ–ª—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ
    norm_qdrant_scores = {k: normalized_scores[i] for i, k in enumerate(qdrant_scores.keys())}
    norm_bm25_scores = {k: normalized_scores[i + len(qdrant_scores)] for i, k in enumerate(bm25_scores_dict.keys())}

    # 4. –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    final_scores = {}
    for filename in set(norm_qdrant_scores.keys()).union(norm_bm25_scores.keys()):
        final_scores[filename] = alpha * norm_qdrant_scores.get(filename, 0) + (1 - alpha) * norm_bm25_scores.get(
            filename, 0)

    # 5. –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞
    sorted_results = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]

    # 6. –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\nüîç **–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫:**")
    for i, (filename, score) in enumerate(sorted_results, 1):
        text = next(p.payload['text'] for p in points if p.payload['filename'] == filename)
        print(f"\n[{i}] üìÑ **{filename}** (Score: {score:.4f})")
        print(f"üìù {text[:300]}...")
        print("-" * 50)




# –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ–∏—Å–∫
query = "Brown coal mining"
search_qdrant_and_bm25(query)
hybrid_search(query, alpha=0.7, top_k=5)
